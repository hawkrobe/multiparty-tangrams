---
title: "Two's company but six is a crowd: emergence of conventions in multiparty communication games"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"
header-includes:
  - \usepackage{booktabs}
author-information: > 
    \author{{\large \bf Veronica Boyce (vboyce@stanford.edu)} \\ Department of Psychology \\ Stanford University
    \AND {\large \bf Robert Hawkins (rdhawkins@princeton.edu)} \\ Neuroscience Institute \\ Princeton University
    \AND {\large \bf Noah Goodman (ngoodman@stanford.edu)} \\ Departments of Computer Science, Psychology \\ Stanford University
    \AND {\large \bf Michael C. Frank (mcfrank@stanford.edu)} \\ Department of Psychology \\ Stanford University}

abstract: >
  From classrooms to dinner parties, many of our everyday conversations take place in larger groups where speakers address multiple listeners at once. Such multiparty settings raise a number of challenges for classical theories of communication, which have largely focused on dyadic interactions. In this study, we investigate how speakers adapt their referring expressions over time as a function of the feedback they receive from multiple parties.  We collected a large corpus of multiparty repeated reference games (98 games, 390 participants, 116K words) where speakers designed referring expressions for groups of 1 to 5 listeners.  We find that speakers initially produce longer utterances when addressing more listeners, but that groups are able to converge to more efficient conventions regardless of the number of listeners. These findings contribute to understanding how shared viewpoints and concepts develop. TODO fix bigger picture statement

    
keywords: >
    Communication; Reference game; Convention; Reduction;
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---
 
# Introduction

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(tidyverse)
library(jsonlite)
library(here)
library(rlang)
library(lme4)
library(brms)
library(rstan)
library(viridis)
library(cowplot)
library(tidybayes)
library(kableExtra)
```



```{r set-up, include=F}
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
theme_set(theme_bw())

ParseJSONColumn <- function(x) {
  str_c("[ ", str_c(x, collapse = ",", sep=" "), " ]")  %>% 
    fromJSON(flatten = T)
}

##Data import constants
data_location="data/study1"




image_location="write-ups/images"

model_location="code/models"


#we exclude rounds where no one talked (this is our criteria for "they weren't playing anymore")
# this is better than if no one submitted b/c maybe people fail to click on some round?
rounds_include <- read_rds(here(data_location,"rounds_include.rds"))

d.round_results <-  read_rds(here(data_location,'round_results.rds'))

# b/c countCorrect was being bad
d.correct <- d.round_results %>% 
  group_by(`_id`,gameId,target,targetNum,repNum,trialNum,
           numPlayers,countCorrect,speaker,tangram) %>% 
  summarize(realCorrect=sum(ifelse(correct,1,0)))


#d.exit_survey <- read_csv(here(data_location,"exit_survey.csv"))

show_summary <- function(model){
  intervals <- gather_draws(model, `b_.*`, regex=T) %>% mean_qi()
  
  stats <- gather_draws(model, `b_.*`, regex=T) %>% 
    mutate(above_0=ifelse(.value>0, 1,0)) %>% 
    group_by(.variable) %>% 
    summarize(pct_above_0=mean(above_0)) %>% 
   mutate(`P-value equivalent` = signif(2*pmin(pct_above_0,1-pct_above_0), digits=2)) %>% 
    left_join(intervals, by=".variable") %>% 
    mutate(lower=round(.lower, digits=2),
           upper=round(.upper, digits=2),
           `Credible Interval`=str_c("[",lower,", ", upper,"]"),
           Term=str_sub(.variable, 3, -1),
           Estimate=round(.value, digits=2)) %>% 
    select(Term, Estimate, `Credible Interval`, `P-value equivalent`)
  
  stats
}

stats <- function(model, row){
  str_c(model[row,1],": Est=", model[row,2], ", CrI=", model[row,3])
}
  
```

Verbal communication is an integral part of our daily lives. We coordinate schedules with partners, socialize with friends over board games, learn and teach in seminar classes, and listen to podcasts. Our communicative environments range in size from one-on-one to small group to large group to broadcast, but the goal of efficient communication is held in common. One necessity for efficient communication is shared reference expressions; when referring to a thing or an idea, it needs some sort of name that the interlocutors will jointly understand. In many cases, there are widely shared conventionalized expressions, but in other cases, spontaneous ad-hoc expressions are needed.

The formation of these new reference expressions is well-studied in dyadic contexts; however, dynamics may be different in larger groups, which are less studied. Our current work builds on the dyadic reference game tradition by extending it to small groups.

## Multi-party communication
The challenge of for speakers when there are multiple listeners is that they must choose utterances that will be effective to listeners who may have different background knowledge. One possibility is that speakers 'aim low', trying to be understood by all parties independently, leading to longer descriptions that focus on the quirks of the listeners who are struggling the most. For example, if a speaker is trying to communicate with an experienced listener and a naive listener, they use longer, more elaborated descriptions than if they were only communicating with an experienced listener [@yoonAdjustingConceptualPacts2014]. This suggests that communication will be slowed down when more listeners are present.

Another possibility is that speakers integrate across listeners and balance efficiency with informativeness by 'aiming in the middle'. When some of the listeners are naive and need longer descriptions, speakers tailor utterances to them; however, speakers make fewer accommodations when there are 3 knowledgeable listeners and only 1 naive listener compared to when the fraction of naive listeners is higher [@yoonAudienceDesignMultiparty2019]. This suggests that there may be non-linear relationships between group size and communication speed, as the speaker aims to be informative to most listeners, but without necessarily accomodating outlier listeners. 

Communication can be slower and more tense when there are disagreements on how to conceptualize certain referents. When a speaker-listener pair was merged with an additional listener from another pair, the new listener had different expectations about what the key features of the referents were [@weberCulturalConflictMerger2003]. After the addition of the new listener, there was a jump in how long it took either listener to make a selection; this slowness persisted for several rounds. @weberCulturalConflictMerger2003 reported that some new listeners expressed frustration when speakers were describing images in terms of features other than the ones they were used to. The larger the group, the more difficult it may be for everyone to agree on a common conceptualization. 

Work on multi-party communication strongly suggests that more people will make communication slower and more elaborated, but this work has focused on the addition of a new listener into a pair or group that had built up some shared representations. This leaves open the question of how group size influences the speed of representation formations to start. 

## Convention formation
Over the course of repeated reference to the same set of images with the same partner, people develop short-hand conventional names for each object. Early descriptions are long and make reference to multiple features in the image, but in later iterations, definite shorthand names are used [@clarkReferringCollaborativeProcess1986]. Recently, online participant recruitment and web-based experiments have made it possible to study this convergence in larger populations using a text-based communication interface. In @hawkinsCharacterizingDynamicsLearning2020, 83 pairs completed a similar iterated reference experiment where they communicated via a chat box. Speakers produced fewer words per image in later blocks than in earlier blocks, in line with results from face-to-face, oral paradigms. 

In general, listeners expect speakers to maintain conventions and stick to descriptions that were similar to successful descriptions. However, they are not suprised to hear different descriptions of a familiar object if it comes from a new speaker who just entered the room [@metzingWhenConceptualPacts2003] or if a new listener is present[@yoonAdjustingConceptualPacts2014]. The reduction pattern may depend on the listener assuming that similar (but shorter) descriptions will correspond to the same image, and new descriptions will not correspond to images with other shorthand names. It's unclear how these expectations map onto groups where the listener and speaker roles rotate, but all participants are involved the entire time. Will later speakers be expected to maintain and develop previously established descriptions? Will the presence of other, possibly confused, listeners license changes in descriptions?  

## Present work
To address some of these questions, we extend the dyadic repeated reference game paradigm of @hawkinsCharacterizingDynamicsLearning2020 to games for 2-6 players who rotate between speaker and listener roles. We compare accuracy and reduction rates in groups of different sizes. This paradigm allows us to confirm that that these findings in dyads extend to larger groups. 

1. Accuracy will increase across blocks.

2. Listeners will respond faster in later blocks.

3. Speakers will reduce their utterances (produce fewer words) in later blocks.

Additionally, we will be able to test for trends across group size on the following two questions.  

4. Do smaller groups use shorter utterances and reduce faster than larger groups?  

5. Is there more overlap in how speakers describe each image in smaller or larger groups? 


```{r chat}

rounds_include <- read_rds(here(data_location,"rounds_include.rds"))
d.round_results <- read_rds(here(data_location,"round_results.rds"))
d.chat.filter <- read_csv(here(data_location, "filtered_chat.csv")) %>% 
  filter(!is.chitchat) %>% 
  filter(!is.na(target)) %>% 
  mutate(text = gsub("\\n", '', fixed = T, spellchecked), # note that this is using spellcorrected version!!!!
         text = gsub("[/?/.]", ' ', text),
         text = str_squish(text),
         tangram = gsub('/experiment/tangram_', '', target, fixed=TRUE),
         tangram = gsub('.png', '', tangram, fixed=TRUE),
         utt_length_chars = str_length(text), 
         utt_length_words = str_count(text, "\\W+") + 1) %>%
  group_by(gameId, trialNum, repNum, tangram) %>% 
  mutate(is.firstutter=ifelse(role!="speaker",F,NA)) %>% 
  fill(c("is.firstutter"), .direction="down") %>% 
  mutate(is.firstutter= is.firstutter %|% T) 

d.chat <- d.chat.filter %>% 
  group_by(gameId, trialNum, repNum, tangram, playerId, role, numPlayers) %>%
  summarize(text = paste0(text, collapse = ', '),
            total_num_words = sum(utt_length_words, na.rm=T) %>% as.numeric(),
            total_num_chars = sum(utt_length_chars, na.rm=T) %>% as.numeric()) %>%
  inner_join(rounds_include) %>% 
  full_join(d.round_results, c("gameId", "trialNum", "repNum", "playerId", "tangram", "numPlayers")) %>% 
  mutate(text = text %|% "",
         total_num_words= total_num_words %|% 0,
         total_num_chars= total_num_chars %|% 0,
         role = role %|% "listener")

d.chat.pre <- d.chat.filter %>% group_by(gameId, trialNum, repNum, tangram, playerId, is.firstutter,role, numPlayers) %>%
  summarize(text = paste0(text, collapse = ', '),
            total_num_words = sum(utt_length_words, na.rm=T) %>% as.numeric(),
            total_num_chars = sum(utt_length_chars, na.rm=T) %>% as.numeric()) %>%
  inner_join(rounds_include) %>% 
  ungroup() %>% 
  mutate(text = text %|% "",
         total_num_words= total_num_words %|% 0,
         total_num_chars= total_num_chars %|% 0,
         role = role %|% "listener") %>% 
  filter(is.firstutter)

```

```{r count}
# For abstract (and elsewhere) count things!

games <- d.round_results %>% select(gameId) %>% unique() %>% nrow() # 98

players <- d.round_results %>% select(gameId, numPlayers) %>% unique() %>% summarize(players=sum(numPlayers)) # 390

words <- d.chat %>% ungroup() %>% select(total_num_words) %>% summarize(words=sum(total_num_words)) #116000

```



# Methods

Building on the methods of @hawkinsCharacterizingDynamicsLearning2020, we used Empirica [@almaatouqEmpiricaVirtualLab2020] to create real-time multi-player reference games. In each game, one of the players started as the speaker who saw an array of tangrams with one highlighted (Figure \ref{game} and communicated which figure to click to the other players (listeners). After the speaker had identified each of the 12 images in turn, the speaker role rotated to another player and the process repeated with the same images. In total, there were 6 blocks, giving each player at least one chance to be the speaker.   We recorded what participants said in the chat, as well as who selected what image and how long they took to make their selections. Code to run the experiment, as well as data and analysis code are available at TODO anonymous OSF clone. <!--at https://github.com/vboyce/FYP-->. 

We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study. Our preregistration is at https://osf.io/cn9f4. TODO check anonymity

## Participants

```{r parts, results="asis"}

summary <- d.round_results %>% group_by(trialNum, repNum, gameId, numPlayers) %>% 
           mutate(time= time %|% 180) %>% 
  summarize(max_time=max(time)) %>% 
  group_by(gameId, numPlayers) %>% 
  summarize(total_time=sum(max_time)/60,
            num_trials=max(trialNum)) %>% 
  arrange(numPlayers) %>% 
  mutate(complete=ifelse(num_trials==71,T,F)) %>% 
  group_by(numPlayers,complete) %>% 
  tally() %>% 
  pivot_wider(names_from=complete, values_from=n) %>% 
  rename(Players=numPlayers,Complete=`TRUE`, Partial=`FALSE`) %>% 
    mutate(Players=int(Players)) %>% 
  arrange(Players,Complete,Partial)

tab1 <- xtable::xtable(summary, align="lccc",
                       caption = "Number of games run for each player count.\\label{parts}")

print(tab1, type="latex", comment = F, include.rownames=F, table.placement = "H")
```

Participants were recruited using the Prolific platform between May and July 2021. We screened for participants who were fluent native English speakers. Participants were paid \$7 for 2-player games, \$8.50 for 3-player games, \$10 for 4-player games, and \$11 for 5- and 6-player games (with the intention of a \$10 hourly rate), in addition to up to \$2.88 in performance bonuses. A total of 390 people participated. 

## Materials
We used the 12 tangram images used by @hawkinsCharacterizingDynamicsLearning2020 and @clarkReferringCollaborativeProcess1986 (see Figure \ref{game}). These images were displayed in a grid with order randomized for each participant (thus descriptions such as "top left" were ineffective as the image might be in a different place on the speaker's and listeners' screens). The same images were used every block. 

## Procedure

We implemented the experiment using Empirica, a Javascript-based platform for running real-time interactive experiments online [@almaatouqEmpiricaVirtualLab2020].  From Prolific, participants were directed to our website where they navigated through a self-paced series of instruction pages explaining the game. Participants had to pass a quiz to be able to play the game. They were then directed to a "waiting room" screen until their partners were ready.

```{r game, fig.env = "figure", fig.pos = "H", fig.align='center', out.width="100%", set.cap.width=T, num.cols.cap=1,  fig.cap = "Screenshot of the speaker's view. Participants see all 12 tangram images.\\label{game}"}
img <- png::readPNG(here(image_location, "interface.PNG"))
grid::grid.raster(img)
```

```{r feedback, fig.env = "figure", fig.pos = "H", fig.align='center', out.width="100%", fig.width=10,fig.height=6, set.cap.width=T, num.cols.cap=1,  fig.cap = "Screenshots of feedback for speakers and listners. Speakers (A) saw what figure each person chose, indicated by the matching icons. Listeners only learned if their selection was correct (B) or incorrect (C). Listeners were not shown what other listeners chose. \\label{feedback}"}
speaker <- png::readPNG(here(image_location, "speaker_feedback.png"))
l_correct <- png::readPNG(here(image_location,"listener_correct.png"))
l_wrong <- png::readPNG(here(image_location,"listener_wrong.png"))

ggdraw()+
  draw_image(speaker, scale=.45, x=.1, y=0, halign = 0, valign=.5)+
    draw_image(l_correct, scale=.45, x=.5, y=.5, halign = 0, valign=0)+
    draw_image(l_wrong, scale=.45, x=.5, y=0, halign = 0, valign=0)+
  draw_label("A", x=.1, y=.75, size=14)+
    draw_label("B", x=.5, y=.8, size=14)+
    draw_label("C", x=.5, y=.2, size=14)


```

Once the game started, participants saw screens like Figure \ref{game}. Each trial, the speaker described the highlighted tangram image so that the listeners could identify and click it. All participants were free to use the chat box to communicate, but listeners could only click once the speaker had sent a message. Once a listener clicked, they could not change their selection. There was no signal to the speaker or other listeners about who had already made a selection. 

### Feedback
Once all listeners had selected (or a 3-minute timer ran out), participants were given feedback (Figure \ref{feedback}). Listeners learned whether they individually had chosen correctly or not; listeners who were incorrect were not told the correct answer. The speaker saw which tangram each listener had selected, but listeners did not. In contrast with prior work  [@hawkinsCharacterizingDynamicsLearning2020] that told all listeners the right answer during the feedback stage, as we were concerned that listeners could learn arbitrary mappings just based on the pairing of what the speaker said and what turned out to be the right answer, without understanding the convention. Without this backchannel, listeners would need to learn the conventions just via the communication channel, which provides a stricter test of how well a group can converge on a convention.

Listeners got 4 points for each correct answer; the speaker got points equal to the average of the listeners' points. These points translated into performance bonus at the end of the experiment. 

In each block, each of the 12 tangrams was indicated to the speaker once. The same person was the speaker for an entire block, but participants rotated roles between blocks. Thus, over the course of the 6 blocks, participants were speakers 3 times in 2-player games, twice in 3-player games, once or twice in 4 and 5-player games, and once in 6-player games. Rotating the speaker was chosen to keep participants more equally engaged (the speaker role is more work), and to give a more robust test for reduction and convention. 

After the game finished, participants were given a survey asking for optional demographic information and feedback on their experience with the game. 

## Data pre-processing and exclusions

Participants could use the chat box freely, which meant that the chat transcript contained some non-referential language. The first author skimmed through the chat transcripts, tagging utterances that did not refer to the current tangram. These were primarily pleasantries ("Hello"), meta-commentary about how well or fast the task was going, and confirmations or denials ("ok", "got it", "yes", "no"). We exclude these utterances from our analyses. Note that chat lines sometimes included non-referential words in addition to words referring to the tangrams ("ok, so it looks like a zombie", "yes, the one with legs"); these lines were retained intact. 

Our intended sample size was 20 complete games in each group size, but we ended up with fewer as shown in Table \ref{parts}.  We excluded incomplete blocks from analyses, but included complete blocks from partial games. (Partial games occurred when a participant disconnected early, for example due to internet trouble.)

# Results

Our first set of research questions were whether the classic findings of accuracy, speed, and reduction that are characteristic of two-player repeated reference games generalized to larger groups. 

## Accuracy and Speed

### Accuracy is high and increasing.

```{r model-acc, include=F}
acc_input <- d.round_results %>% group_by(playerId,repNum, gameId, numPlayers) %>% 
  mutate(correct.num=ifelse(correct,1,0)) %>% 
  mutate(block=repNum,
         tangram_group=str_c(tangram, gameId))

priors <- c(set_prior("normal(0,1)", class="b"))#, #we're doing logistic, so these are reasonable b/c transform
                     # set_prior("normal(0,1)", class="sd"))

model <- brm(correct.num ~ block*numPlayers, 
             family=bernoulli(link="logit"),
             data=acc_input, 
             file=here(model_location, "acc_model"), prior=priors, control=list(adapt_delta=.95))

acc <- show_summary(model) %>% write_rds(here(model_location,"acc_summ.rds"))
```

```{r}
acc <- read_rds(here(model_location,"acc_summ.rds"))
```

```{r accuracy, fig.env="figure", fig.pos = "H", fig.align = "center", out.width="100%", fig.width=5, fig.height=3, fig.cap = "Player's accuracy at correctly selecting the target figure by block and group size. Accuracy increases across blocks. \\label{accuracy}" }
d.round_results %>% group_by(playerId,repNum, gameId, numPlayers) %>% 
  mutate(correct.num=ifelse(correct,1,0)) %>% 
  ggplot(aes(x=repNum+1, y=correct.num, color=as.factor(numPlayers)))+
geom_smooth(method = "glm", method.args = list(family = "binomial"), size=1.3) + 
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width=.3), size=.5)+
  scale_color_viridis(discrete=T, direction=-1)+
    scale_x_continuous(breaks=seq(1,6))+
  labs(x="Block", y="Selection Accuracy", color="Players")+
  theme(legend.position="right")

```

Most individuals were accurate in their selections, with accuracy rising across blocks (Figure \ref{accuracy}).  In a logistic model of accuracy, participants are more accurate in later blocks (`r stats(acc,1)`), and there was no strong effect of group size on accuracy (`r stats(acc,4)`) or interaction between block and group size (` r stats(acc,2)`). 

### Participants speed up in later blocks.

```{r model-time, include=F}
#note: there were some issues with time recording, source unknown. We exclude the obvious errors where time > 180, but it's probably still a bit dodgy?
time_input <- d.round_results %>% group_by(playerId, repNum, gameId, numPlayers) %>% 
  filter(correct==T) %>% 
  filter(time<181) %>% 
  filter(time>0) %>% 
  mutate(block=repNum,
         tangram_group=str_c(tangram, gameId))

priors <- c(set_prior("normal(0,100)", class="Intercept"),
            set_prior("normal(0,50)", class="b"))

model <- brm(time ~ block*numPlayers , 
             data=time_input, 
             file=here(model_location, "time_model"), prior=priors, control=list(adapt_delta=.95))

time <- show_summary(model) %>% write_rds(here(model_location,"time_summ.rds"))
```

```{r}
time <- read_rds(here(model_location,"time_summ.rds"))
```


```{r time, fig.env="figure", fig.pos = "H", fig.align = "center",out.width="100%", fig.width=5, fig.height=3, fig.cap = "How long listeners took to select a figure in seconds by block and group size. Listeners selected images faster in later blocks. Only times for correct responses are shown.\\label{time}" }
d.round_results %>% group_by(playerId, repNum, gameId, numPlayers) %>% 
  filter(correct==T) %>% 
  filter(time<181) %>% 
  #summarize(time=mean(time)) %>% 
  ggplot(aes(x=repNum+1, y=time, color=as.factor(numPlayers)))+
  geom_jitter(width=.4, height=0, alpha=.03)+
geom_smooth(method = "glm", formula = y~x,
                      method.args = list(family = gaussian(link = 'log')))+
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width=.2))+
  scale_y_continuous(limits = c(0,180))+
      scale_x_continuous(breaks=seq(1,6))+
 scale_color_viridis(discrete=T, direction=-1)+ 
  labs(x="Block", y="Time to select (s)", color="Players")+
  theme(legend.position = "right")

```


Participants selected images faster in later blocks (Figure \ref{time}), although there was wide variability. In a linear model of selection time, participants got faster across blocks (`r stats(time,1)`) and were slightly slower in larger games (`r stats(time,4)`). This speed up is consistent with prior work by @weberCulturalConflictMerger2003 which used speed as the dependent measure. Wide variability in selection time meant that especially for larger groups, there was a wide spread in how long it took groups to complete the experiment. 

## Reduction

The key finding in dyadic reference games is that speakers produce shorter utterances as conventionalized names for the images arise. We replicate this finding in larger groups. Both speakers and listeners reduce the amount they say over the course of blocks. 

### Listeners rarely talk.

```{r model-listener, include=F}
listener_input <- d.chat %>% filter(role=="listener") %>% 
  mutate(block=repNum,
         words=total_num_words,
         tangram_group=str_c(tangram, gameId))
         
priors <- c(
  set_prior("normal(0, 20)", class="Intercept"),
  set_prior("normal(0, 10)", class="b"),
  set_prior("normal(0, 10)", class="sd"),
  set_prior("lkj(1)",       class="cor"))

model <- brm(words ~ block * numPlayers + (block|tangram)+ (1|playerId)+(1|tangram_group)+(block|gameId), data=listener_input,file=here(model_location, "listener_model"),                            prior=priors, control=list(adapt_delta=.95))

         
listener <- show_summary(model) %>% write_rds(here(model_location,"listener_summ.rds"))
```

```{r}
listener <- read_rds(here(model_location,"listener_summ.rds"))
```

Listeners often don't talk much, but are more likely to ask questions or make clarification in early blocks. In a linear regression for the number of words each listener said, there is an effect of block (`r stats(listener, 1)`), but no clear effect of game size (`r stats(listener,4)`). 


### Speakers utterances reduce in length. 

```{r variability, fig.env="figure", fig.pos = "H", fig.align = "center",out.width="100%", fig.width=6, fig.height=3, fig.cap = "Number of words from speaker (total, across all 12 figures) in a block. Each colored line is one group, the overall trend is shown in black. Across group size, the number of words decreases as conventions emerge, but convention formation is not a smooth process, and there is variability between speakers.\\label{variability}" }
d.chat %>% filter(role=="speaker") %>% group_by(gameId,numPlayers,repNum) %>% 
  summarize(words=sum(total_num_words)) %>% ggplot(aes(x=repNum+1, color=as.factor(numPlayers), y=words, ))+geom_line(aes(group=gameId))+facet_wrap(~numPlayers, nrow=1)+
  geom_smooth(method=glm, formula=y~poly(x,2), se=F, color="black")+
  scale_color_viridis(discrete=T, direction=-1)+
  scale_x_continuous(breaks=seq(1,6))+
  labs(y="Total words from speaker", x="Block", color="Players")+
  theme(legend.position="none")

```

```{r model-speaker, include=F}
speaker_input <- d.chat %>% filter(role=="speaker") %>% 
  mutate(block=repNum,
         words=total_num_words,
         tangram_group=str_c(tangram, gameId))
         
priors <- c(
  set_prior("normal(20, 20)", class="Intercept"),
  set_prior("normal(0, 10)", class="b"),
  set_prior("normal(0, 10)", class="sd"),
  set_prior("lkj(1)",       class="cor"))

model <- brm(words ~ block * numPlayers + (block|tangram)+ (1|playerId)+(1|tangram_group)+(block|gameId), data=speaker_input,file=here(model_location, "speaker_model"),                            prior=priors, control=list(adapt_delta=.95))

speaker <- show_summary(model) %>% write_rds(here(model_location,"speaker_summ.rds"))
```

```{r}
speaker <- read_rds(here(model_location,"speaker_summ.rds"))
```

 As shown in Figure \ref{variability}, the number of words produced by speakers decreases over the course of rounds. This is true in aggregate, but also true for many groups. Nonetheless, in some groups, a later speaker may be more verbose than an earlier speaker. Speakers make longer utterances in early blocks that reduce to shorter utterances in later blocks. From a linear model, the effect of being one block later is `r stats(speaker,1)`. 
 
### Larger groups say more. 

One open question about larger groups was whether they would produce longer utterances or reduce more slowly than speakers in smaller groups. The overall effect of having more players in a group is `r stats(speaker,4)` per additional player. There is no clear interaction between block and group size `r stats(speaker,2)`. This is consistent with predictions from audience design that with more listeners to accommodate, the speaker may hedge with multiple conceptualizations. Larger groups also have more listeners who may ask clarifying questions that the speaker feels the need to respond to. 




## Development of conventions 
One broad question is how speakers decide when to follow a previously established description and repeat it, perhaps in a reduced form, and when to use a different conceptualization. 

### When speakers don't know the convention

```{r speaker-acc, include=F}
d.prev.speaker <- d.chat %>% ungroup() %>%  filter(role=="speaker") %>% select(gameId,repNum, tangram, total_num_words_prev=total_num_words)
d.prev.round <- d.chat %>% ungroup() %>% select(playerId, correct, tangram, gameId, repNum) %>% 
  left_join(d.prev.speaker) %>% unique() %>% mutate(repNum=repNum+1)


d.chat.lagged <- d.chat %>%
  ungroup() %>% 
  select(gameId, playerId, trialNum, repNum, playerId, role, tangram, total_num_words, numPlayers) %>%
  left_join(d.prev.round) %>%
  mutate(reduction_word=log(total_num_words)-log(total_num_words_prev)) %>%
  filter(repNum>0) %>%
  filter(role=="speaker") %>%
  mutate(prev_correct_round=correct)


model_input <- d.chat.lagged %>% filter(role=="speaker") %>% 
  mutate(block=repNum,
         words=total_num_words,
         tangram_group=str_c(tangram, gameId),
         was_INcorrect=ifelse(!prev_correct_round,1,0))
         
priors <- c(
  set_prior("normal(20, 20)", class="Intercept"),
  set_prior("normal(0, 10)", class="b"),
  set_prior("normal(0, 10)", class="sd"),
  set_prior("lkj(1)",       class="cor"))

model_speaker_acc <- brm(words ~ block * numPlayers +block*was_INcorrect+ (block|tangram)+ (1|playerId)+(1|tangram_group)+(block|gameId), data=model_input,file=here(model_location, "speaker_acc"),                         prior=priors, control=list(adapt_delta=.95))

speaker_acc <- show_summary(model_speaker_acc) %>% write_rds(here(model_location,"speaker_acc_summ.rds"))
```

```{r}
speaker_acc <- read_rds(here(model_location,"speaker_acc_summ.rds"))
```


In these games with limited feedback, listeners who got a tangram wrong don't have a way of knowing what the right answer was unless they ask for clarification in the chat. This means that if a speaker got a tangram wrong as a listener in the previous block, they may not know the conventional description that does with it, and thus are unlikely to follow it.  If we assume that reduction is a sign of convention development, this predicts speakers should say more words when they got the tangram wrong the previous block, after controlling for other effects. This is borne out; speakers say more words for tangrams after they were incorrect (`r stats(speaker_acc,6)`). 

### Where do conventions come from?

```{r similarity, fig.env="figure", fig.pos = "H", fig.align = "center",out.width="100%", fig.width=6, fig.height=3, fig.cap = "The fraction of content words used by the speaker in the final (6th) block that were used to describe the same tangram in each earlier block. Overlap is higher in smaller games.\\label{similarity}" }
d.numPlayer <- read_rds(here(data_location,'round_results.rds')) %>% select(numPlayers, gameId)
matches <- read_csv(here("data/study1/word_matches.csv")) %>% left_join(d.numPlayer, by="gameId")



location_first_match <- matches %>% 
  filter(later_rep==5) %>% 
  group_by(earlier_rep,gameId,numPlayers) %>% 
  summarize(overlap=mean(match))

ggplot(location_first_match, aes(x=earlier_rep+1, y=overlap, color=as.factor(numPlayers)))+
  geom_jitter(alpha=.5,width=.2, height=0)+facet_grid(.~numPlayers)+
    scale_color_viridis(discrete=T, direction=-1)+
    stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width=.2), color="black")+
  labs(x="Earlier block", y="Overlap")+
  theme(legend.position="none")



```

```{r same-speaker, include=F}
same_speaker <- matches %>% 
  filter(later_rep==5) %>% 
  mutate(same_speaker=ifelse(earlier_rep%%numPlayers==later_rep%%numPlayers,1,0),
         match.num=ifelse(match, 1,0)) %>% 
    mutate(target=str_extract(target,"[A-Z]")) %>% 
  select(gameId,target,block=earlier_rep,numPlayers,match, same_speaker) %>% 
  group_by(gameId,target,block, numPlayers, same_speaker) %>% 
  summarize(overlap=mean(match))
  
  priors <- c(
  set_prior("normal(0, 1)", class="Intercept"),
  set_prior("normal(0, 1)", class="b"),
  set_prior("normal(0,1)", class="sd"))
  #set_prior("lkj(1)",       class="cor")
  #)

 model_conventions <- brm(overlap ~ block * numPlayers + same_speaker + (1|gameId)+(1|target), data=same_speaker,file=here(model_location, "model_convention"),  prior=priors, control=list(adapt_delta=.95))

  #model_conventions <- glm(match ~ block * numPlayers + same_speaker, data=same_speaker ,family=binomial)
         
conventions <- show_summary(model_conventions) %>% write_rds(here(model_location,"conventions_summ.rds"))
```

```{r}
conventions <- read_rds(here(model_location,"conventions_summ.rds"))
```


[TODO: check methods] Another angle to look at conventions is to take the speaker's utterances in the last block as the "convention", and look at how far back they started. To do this, we took the contentful words from the speakers last block utterances and looked at how many of them were used to describe that tangram in prior rounds. This let us calculate a fraction overlap between the last round utterance and earlier rounds, shown in Figure \ref{similarity}. In a linear model of the overlap between an earlier block and the last block, later blocks have more overlap `r stats(conventions,1)`.  Blocks with the same speaker as the last round have more overlap `r stats(conventions,5)`; this is easiest to see in the peaks for rounds 2 and 4 in the 2 player games. Larger groups lead to less overlap between blocks `r stats(conventions,4)`, but there is no interaction between blocks and game size `r stats(conventions,2)`. Overall, this suggests that in smaller groups, conventions reduce and stabilize sooner, perhaps because fewer people need to implicitly agree on them. One potential confound is that in smaller games, players spend more time in the speaker role; however, there is still more overlap in smaller games even to blocks with a different speaker. 


### Examples

While most groups did form conventions for most tangrams, it's illustrative to look at what happens when this doesn't happen. Table \ref{diamond} shows the transcript of a 4-person groups for a specific figure where they described it geometrically every round, leading to long, and not very informative descriptions. Nearly all the figures have diamond heads, so this isn't a distinguishing feature, yet it is described. This illustrates the variability between groups, but also why conventions might be useful. 

```{r diamond}
diamond <- tribble(~`Block`, ~`Person`, ~Text,
                 "1","A(S)","Diamond on top. Body with no real arms or legs. The body is shaped like a boot with the diamond on top.",
		"","C", "Is the boot pointed left or right?",
		"2", "B(S)", "diamond on top, large body beneath it. Left is a straight line all the way down, small variations on the right to the main body",
		"3", "C(S)", "Diamond in center on top. Left side straight, right side carved out like a vase.",
		"4", "D(S)", "Diamond head, flat topped body, straight on the left side with two triangles pointing out on the left",
		"","D(S)" ,"*on the right",
	"5" ,"A(S)", "Diamond on top. Left side is straight, right side is obstructed, looks like a boot",
	"", 	"B", "what do you mean by obstructed?",
		"", "A(S)", "The left side of the body is right, right side has bents in it",
		"6", "B(S)", "Diamond on top of a long large body/rectangle. Left side is complete, right side has bits missing")

knitr::kable(diamond, caption = "Excerpt from a group that did not reduce very much. The speaker for each round is marked with (S). Figure under discussion is row 3, column 3 in Figure \\ref{game}.\\label{diamond}",  format="latex", booktabs=TRUE) %>% 
kable_styling(full_width = F) %>%
column_spec(3, width = "16em")

```

A different 4-person group had a member who during the first block shared the idea that the task would be easier if they explicitly gave "codenames" to the figures. The transcript for this group and one of the tangrams in shown in Table \ref{zigzag}. Of note, multiple speakers forget the assigned codename, demonstrating that meta-knowledge doesn't always help. This group also describes the figure in relation to another already-named figured. Nonetheless, the group successfully conventionalizes on a couple reduced names for this figure: "zigzag" and "beggar". This dual-naming of figures from multiple conceptual angles contributed by different speakers also occurs in other games. 

```{r zigzag}

zigzag <- tribble(~Block, ~Person, ~`Text`,
 "1", "A(S)" , "[...] yes, the legs are like a zig zag",
		"",	"C", "CODE name ZIGZAG",
			"","A(S)",	"There are no legs upwards",
			"2", "B(S)", "okay so similar to begger guy but no foot pointing up",
			"","B(S)", "its like a zigzag",
			"","B(S)", "i forgot the code name",
			"","D", "zigzag yea",
			"","A", "The one standing with knees bent",
			"","B(S)", "yeah",
			"","B(S)" ,"standing",
			"", "C", "Yeah zigzag",
			"3", "C(S)",	"The begger with no foot coming out from the left",
			"","B", "zigzag",
			"","C(S)",	"zigzag it is",
			"","C(S)",	"sorry i forgot",
			"4", "D(S)",	"zigzag",
		"5", "A(S)",	"zigzag",	
			"6", "B(S)",	"beggar guy",
			"", "B(S)" ,"zigzag")

knitr::kable(zigzag, caption = "Excerpt from a group that explicitly gave nicknames to the figures.  The speaker for each round is marked with (S). Figure under discussion is row 1, column 4 in Figure \\ref{game}. \\label{zigzag}", format="latex",booktabs = TRUE)%>%
column_spec(3, width = "16em")


```

# Discussion

The overall patterns present in dyadic repeated reference games generalize to small groups of participants. Selection accuracy increases over rounds at the same time as listeners speed up their selections. Speakers reduce the length of their descriptive utterances as they conventionalize on concepts for each image.

Group size does make a difference, as larger groups talk more and are slower than smaller groups. There is also more diversity in how images are described within larger groups. This could just be from slower convergence to a conventions, or from parallel competing conceptualizations favored by different speakers. 

The patterns of reduction are robust to speaker rotation and the lack of gold-standard feedback to listeners. Not only do speakers say less in later repetitions than they themselves said earlier, speakers later in the order say less than speakers earlier in the rotation.   

## Future work
There are a number of design parameters in this experiment that could effect the patterns of reduction. It remains to future work to vary these parameters and determine which are relevant. Luckily, with an online implementation, recruiting for and running experiments is relatively easy, and thus it will be possible to iterate on this experiment to determine how far the patterns generalize. 

We provided less feedback than previous studies such as @hawkinsCharacterizingDynamicsLearning2020. This low level of feedback means that there isn't a way for people to find out what was meant for utterances they initially did not understand outside of the verbal communication channel (or process of elimination). Similarly, speakers don't have direct access to how well their partners did in the previous block. Real-life communicative situations vary in what extra-textual feedback exists, but we do show that people can work around their initial confusion to eventually understand utterances, rather than just memorizing pairings after the first occurrence. These were fairly unfavorable settings where we limited the amount of information flow. Future experiments could see whether making more things public by making selections visible to all during the feedback stage changes the amount of reduction observed. 

Additionally, while each participant had a nickname, color, and icon to identify them, individual identity was not especially salient. Future experiments could make the individual identity of each participant more salient by showing their responses in a consistent location on the screen, or allowing participants to get to know each other first. Different design choices in the interaction provide simuations for different contexts in which reference is established in nomal life.

This is a rich data set that  may yield more information about how humans adapt language quickly, and the dataset may be useful for training artificial agents to use and understand language more dynamically. 

<!--
# General Formatting Instructions 

For general information about authoring in markdown, see **[here](http://rmarkdown.rstudio.com/authoring_basics.html).**

The entire content of a paper (including figures, references, and anything else) can be no longer than six pages in the \textbf{initial submission}. In the \textbf{final submission}, the text of the paper, including an author line, must fit on six pages. Up to one additional page can be used for acknowledgements and references.

The text of the paper should be formatted in two columns with an
overall width of 7 inches (17.8 cm) and length of 9.25 inches (23.5
cm), with 0.25 inches between the columns. Leave two line spaces
between the last author listed and the text of the paper; the text of
the paper (starting with the abstract) should begin no less than 2.75 inches below the top of the
page. The left margin should be 0.75 inches and the top margin should
be 1 inch.  \textbf{The right and bottom margins will depend on
whether you use U.S. letter or A4 paper, so you must be sure to
measure the width of the printed text.} Use 10~point Times Roman
with 12~point vertical spacing, unless otherwise specified.

The title should be in 14~point bold font, centered. The title should
be formatted with initial caps (the first letter of content words
capitalized and the rest lower case). In the initial submission, the
phrase ``Anonymous CogSci submission'' should appear below the title,
centered, in 11~point bold font.  In the final submission, each
author's name should appear on a separate line, 11~point bold, and
centered, with the author's email address in parentheses. Under each
author's name list the author's affiliation and postal address in
ordinary 10~point type.

Indent the first line of each paragraph by 1/8~inch (except for the
first paragraph of a new section). Do not add extra vertical space
between paragraphs.


# First-Level Headings

First level headings should be in 12 point , initial caps, bold and
centered. Leave one line space above the heading and 1/4~line space
below the heading.

## Second-Level Headings

Second level headings should be 11 point , initial caps, bold, and
flush left. Leave one line space above the heading and 1/4~ line
space below the heading.

### Third-Level Headings

Third-level headings should be 10 point , initial caps, bold, and flush
left. Leave one line space above the heading, but no space after the
heading.

# Formalities, Footnotes, and Floats

Use standard APA citation format. Citations within the text should
include the author's last name and year. If the authors' names are
included in the sentence, place only the year in parentheses, as in
[-@NewellSimon1972a], but otherwise place the entire reference in
parentheses with the authors and year separated by a comma
[@NewellSimon1972a]. List multiple references alphabetically and
separate them by semicolons [@ChalnickBillman1988a; @NewellSimon1972a]. 
Use the et. al. construction only after listing all the authors to a
publication in an earlier reference and for citations with four or
more authors.

For more information on citations in RMarkdown, see **[here](http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html#citations).**

## Footnotes

Indicate footnotes with a number\footnote{Sample of the first
footnote.} in the text. Place the footnotes in 9 point type at the
bottom of the page on which they appear. Precede the footnote with a
horizontal rule.\footnote{Sample of the second footnote.} You can also use 
markdown formatting to include footnotes using this syntax [^1].

[^1]: Sample of a markdown footnote.

## Figures

All artwork must be very dark for purposes of reproduction and should
not be hand drawn. Number figures sequentially, placing the figure
number and caption, in 10 point, after the figure with one line space
above the caption and one line space below it. If necessary, leave extra white space at
the bottom of the page to avoid splitting the figure and figure
caption. You may float figures to the top or bottom of a column, or
set wide figures across both columns.

## Two-column images

You can read local images using png package for example and plot 
it like a regular plot using grid.raster from the grid package. 
With this method you have full control of the size of your image. **Note: Image must be in .png file format for the readPNG function to work.**

You might want to display a wide figure across both columns. To do this, you change the `fig.env` chunk option to `figure*`. To align the image in the center of the page, set `fig.align` option to `center`. To format the width of your caption text, you set the `num.cols.cap` option to `2`.

```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=4, fig.height=2, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "This image spans both columns. And the caption text is limited to 0.8 of the width of the document."}
img <- png::readPNG("figs/walrus.png")
grid::grid.raster(img)
```

## One-column images

Single column is the default option, but if you want set it explicitly, set `fig.env` to `figure`. Notice that the `num.cols` option for the caption width is set to `1`.

```{r image, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=2, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "One column image."}
img <- png::readPNG("figs/lab_logo_stanford.png")
grid::grid.raster(img)
```


## R Plots

You can use R chunks directly to plot graphs. And you can use latex floats in the
fig.pos chunk option to have more control over the location of your plot on the page. For more information on latex placement specifiers see **[here](https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions)**

```{r plot, fig.env="figure", fig.pos = "H", fig.align = "center", fig.width=2, fig.height=2, fig.cap = "R plot" }
x <- 0:100
y <- 2 * (x + rnorm(length(x), sd = 3) + 3)

ggplot2::ggplot(data = data.frame(x, y), 
                aes(x = x, y = y)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```


## Tables

Number tables consecutively; place the table number and title (in
10 point) above the table with one line space above the caption and
one line space below it, as in Table 1. You may float
tables to the top or bottom of a column, set wide tables across both
columns.

You can use the xtable function in the xtable package.

```{r xtable, results="asis"}
n <- 100
x <- rnorm(n)
y <- 2*x + rnorm(n)
out <- lm(y ~ x)

tab1 <- xtable::xtable(summary(out)$coef, digits=c(0, 2, 2, 1, 2), 
                       caption = "This table prints across one column.")

print(tab1, type="latex", comment = F, table.placement = "H")
```
-->
# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
