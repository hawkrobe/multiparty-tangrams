---
title             : "How do conventions emerge in group communication: Evidence from 2-4 player reference games"
shorttitle        : "Conventions in groups"

author: 
  - name          : "Veronica Boyce"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : ""
    email         : "vboyce@stanford.edu"
    role:        ""
 
affiliation:
  - id            : "1"
    institution   : "Stanford University"
 
authornote: |
  First Year Project for Psych Department

abstract: |
  TODO
  
keywords          : ""
wordcount         : ""
header-includes:
 - \usepackage{setspace}\singlespacing
 - \renewcommand{\topfraction}{.85}
 - \renewcommand{\bottomfraction}{.7}
 - \renewcommand{\textfraction}{.15}
 - \renewcommand{\floatpagefraction}{.66}
 - \setcounter{topnumber}{3}
 - \setcounter{bottomnumber}{3}
 - \setcounter{totalnumber}{4}

bibliography      : ["r-references.bib", "FYP.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")

knitr::opts_chunk$set(echo = FALSE, warning=F, message=F)
knitr::opts_chunk$set(dev = "png", dev.args = list(type = "cairo-png"))
options(knitr.table.format = "html")
library(tidyverse)
library(jsonlite)
library(here)
library(rlang)
library(brms)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
theme_set(theme_bw())

ParseJSONColumn <- function(x) {
  str_c("[ ", str_c(x, collapse = ",", sep=" "), " ]")  %>% 
    fromJSON(flatten = T)
}

##Data import constants
data_location="data/study1"

date_start=lubridate::ymd('2021-05-04')

image_location="write-ups/images"

model_location="code/models"
```

```{r, include=F, cache=T}
d.games <- read_csv(here(data_location, 'games.csv')) %>% 
  rename(gameId = `_id`) %>% 
    filter(createdAt >= date_start)

d.chat.raw <- read_csv(here(data_location, 'rounds.csv'), guess_max=10000) %>%
  filter(createdAt >= date_start) %>%
  mutate(data.chat = ifelse(is.na(data.chat), '{}', data.chat)) %>%
  rename(row_id = `_id`) %>%
  mutate(data.chat = map(data.chat, .f = ParseJSONColumn)) %>%
  unnest(data.chat) %>%
  select(-data.target, -ends_with('response'), -ends_with('_correct'), -ends_with('time')) %>%
  rename_with(~ gsub("data.", "", .x, fixed = TRUE)) %>% 
  write_csv(here(data_location, 'raw_chat.csv'))

d.round_results.raw <- read_csv(here(data_location,'rounds.csv'),guess_max=10000) %>% 
  filter(createdAt >= date_start) %>% 
  rename_with(~ gsub("data.", "", .x, fixed = TRUE)) %>% 
  rename_with ( ~ gsub("room", "player", .x, fixed=T)) %>% 
    rename_with ( ~ gsub("player", "player_", .x, fixed=T)) %>% 
    rename_with ( ~ gsub("correct", "_correct", .x, fixed=T)) %>% 
    rename_with ( ~ gsub("response", "_response", .x, fixed=T)) %>% 
  rename_with( ~ gsub("time", "_time", .x, fixed=T)) %>% 
  select(-chat) %>% 
  gather(key, value, starts_with('player')) %>% 
  separate(key, into = c('blah', 'playerId', 'info')) %>% 
  spread(info, value) %>% 
  select(-blah) %>% 
  mutate(tangram = gsub('/experiment/tangram_', '', target, fixed=TRUE),
         tangram = gsub('.png', '', tangram, fixed=TRUE)) %>% 
  mutate(correct=as.logical(correct),
         time=as.numeric(time)/1000) %>% 
  filter(!is.na(correct)) %>% 
  filter(playerId!=speaker)

#only include rounds that finished
rounds_exclude <- d.round_results.raw %>% group_by(gameId,numPlayers,repNum) %>% tally() %>% filter(n!=12*(numPlayers-1)) %>% select(gameId,repNum)

d.round_results <-  d.round_results.raw %>% anti_join(rounds_exclude)

```



# Intro

Verbal communication is an integral part of our daily lives. We coordinate schedules with individuals, socialize with friends over board games, learn and teach in seminar classes, and listen to podcasts. These different communicative envirnoments range in size from one-on-one to small group to larger groups to broadcast communication, but the goal of efficient communication is held in common. One necessity for efficient communication is shared reference expressions; when referring to a thing or an idea, it needs some sort of name that the interlocuters will jointly understand. In many cases, there are widely shared conventionalized expressions, but in other cases, spontaneous ad-hoc expressions need to be create to refer to new or specific things.

The formation of these new reference expressions is well-studied in dyadic contexts; however, dynamics may be different in larger groups, which are less studied. While the range of communicative situations is wide, our current work builds on the dyadic reference game tradition by extending it to small groups.

The classic paradigm for iterated reference games has two partners, a speaker and a listener who see the same set of images in different orders  [@clarkReferringCollaborativeProcess1986]. The goal each round is for the speaker to describe the images so the listener can label their copies with the correct order. After receiving feedback, the pair does the task again with the same images but a new order. Crucially, reference expressions change and shorter over the course of repeated reference to the same image. Early descriptions are long, hesitant and make reference to multiple features or concepts in the image, but by later rounds the figures are often referred to by definite shorthand names. 

The basic result has been repeatedly replicated TODO CITE SO MANY THINGS!!!!. Recently, online participant recruitment and web-based experiments have made it possible to study this convergence to shorthand reference experiments in larger populations using a text-based communication interface. In @hawkinsCharacterizingDynamicsLearning2020, 83 pairs completed a cued version of the iterated reference experiment. On each trial, the speaker saw one image highlighted and described it to the listener who clicked on what they thought the target was. Both players received feedback on how they did and which image was correct before moving on to the next target image. All images were highlighted each block, for a total of 6 blocks. Speakers produced fewer words per image in later blocks than in earlier blocks, confirming the common finding. 

While this reduction pattern is robust for dyads, less is known about the how utterances are adapted in larger groups. A couple of studies point to some potential difficulties in trying to communicate with multiple people at once. 

@yoonAudienceDesignMultiparty2019 had speakers complete a sorting task with some listeners, so that they would have a common ground of shared names for the images. Then in a test phase, the speaker described these images to a group of either all knoweledgeable listeners from the sorting task, new listeners who had not done the sorting task, or a mix of knowledgeable and new listeners. Speakers produced longer and more disfluent utterances when any new listeners were present than with only experienced listeners, but there were also graded effects. This suggests that targeting utterances at an audience of mixed knowledge level is difficult. This might predict slower reduction in larger groups where there will inevitably be some variability in how people understand reference expressions. These studies included 3-hour experiments that were very time and labor intensive, but some of the questions about group dynamics may be addressable in online experiments taking advantage of natural variation in understanding without artifically inducing large knowledge differences.

It's difficult to communicate with naive listeners, but it can be even harder to communicate with someone with entrenched preconceptions that differ. @weberCulturalConflictMerger2003 induced these conceptual differences by having two pairs of people (each pair representing a "firm") do an iterated reference game with the same set of pictures. After 20 rounds, there was a "merger" where the listener from one group joined the other group. The reference game continued with the speaker trying to communicate to both their original listener and the new listener. While over the initial rounds, the time taken to indentify the images declined, after the merger, there was a jump in how long it took either listener to make a selection. Even after several more rounds, listeners were still not as fast as before the merger. With larger groups of people all speaking together, there's a greater chance for different people to independently develop different conceptualizations of an image, and this study suggests it may be difficult for them to understand each other or agree on a common term of reference. 

In this work, we extend the dyadic repeated reference game paradigm of @hawkinsCharacterizingDynamicsLearning2020 to games for 2-4 players which allows us to compare the rates of reduction in groups of different sizes.


# Methods

(ref:game) Screenshot of the speaker's view. Participants see all 12 tangram images.

```{r game, out.height="40%", fig.pos='h', fig.cap="(ref:game)", fig.align="center"}

knitr::include_graphics(here(image_location, "interface.PNG"))


```

We recruited participants to play a repeated reference game in groups of 2-4. Participants viewed an array of 12 tangrams (Fig \@ref(fig:game)). One person was assigned the speaker role and say one of the images highlighted; the goal was for them to communicate the identity of this image to their partners who would then click on it. All participants were free to use the chat box to communicate. The speaker identified each of the 12 tangrams during a block; then the speaker role rotated to a different participant for the next block. Each group completed a total of 6 blocks, all with the same 12 images. We recorded what participants said in the chat, as well as who selected what image and how long they took to select.

We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study. Our preregistration is at https://osf.io/cn9f4.

## Participants

```{r, include=F}
summary <- d.round_results %>% group_by(trialNum, repNum, gameId, numPlayers) %>% 
           mutate(time= time %|% 180) %>% 
  summarize(max_time=max(time)) %>% 
  group_by(gameId, numPlayers) %>% 
  summarize(total_time=sum(max_time)/60,
            num_trials=max(trialNum)) %>% 
  arrange(numPlayers) %>% 
  mutate(complete=ifelse(num_trials==71,T,F)) %>% 
  group_by(numPlayers,complete) %>% 
  tally()
```

Participants were recruited using the Prolific platform between 4th and 10th of May 2021. We screened for participants who were fluent, native English speakers. Participants were paid \$7 for 2-player games, \$8.50 for 3-player games, and \$10 for 4-player games (with the intentional of a $10 hourly rate), in addition to performance bonuses.

Our intended sample size was 20 complete games in each group size, but we ended up with `r summary[2,3]` complete 2-player games (`r summary[1,3]`  partial), `r summary[4,3]` complete 3-player games (`r summary[3,3]`  partial), and `r summary[6,3]` complete 4-player games (`r summary[5,3]`  partial). We included excluded incomplete blocks from analyses, but included complete blocks from partial games. (Partial games occurred when a participant disconnected early, for example due to internet trouble.)

## Materials
We used the 12 tangram images used by @hawkinsCharacterizingDynamicsLearning2020 and @clarkReferringCollaborativeProcess1986 (see Fig \@ref(fig:game)). These images were displayed in a grid for each participant with order randomized for each participant. The same images were used every round. 

## Procedure

We implemented the experiment using Empirica, a Javascript-based platform for running real-time interactive experiments online [@almaatouqEmpiricaVirtualLab2020]. Code for running this experiment is available at https://github.com/vboyce/FYP. From Prolific, participants were directed to our website where they navigated through a self-paced series of instruction pages that explained the game. Participants had to pass a quiz to be able to play the game. They were then directed to a "waiting room" screen until their partners were ready.

Once the game started, participants saw screens like Fig \@ref(fig:game). Each trial, the speaker had to describe the highlighted tangram image so that the listeners could identify it and click it. All participants were free to use the chat box to communicate. Listeners could only click once the speaker had sent a message. Once all listeners has selected (or a 3-minute timer had run out), participants were given feedback. Listeners only learned whether they individually had chosen correctly or not; listeners who were incorrect were not told the correct answer. The speaker saw which tangram each listener had selected. Listeners got 4 points for each correct answer; the speaker got points equal to the average of the listeners points. These points translated into cents of performance bonus at the end of the experiment. 

In each block, each of the 12 tangrams was indicated to the speaker once. The same person was the speaker for an entire block, but participants rotated roles between blocks. Thus, over the course of the 6 blocks, participants were speakers 3 times in 2-player games, 2 times in 3-player games and once or twice in 4-player games.

After the game finished, participants were given a survey asking for optional demographic information and feedback on their experience with the game. 

## Data analysis
I skimmed through the chat transcripts, tagging utterances that did not refer to the current tangram. These were primarily pleasantries ("Hello"), meta-commentary about well or fast the task was going and confirmations or denials ("ok", "got it", "yes", "no"). We exclude these utterances from our analyses.

```{r}
package_list <- c("tidyverse","brms","here","jsonlite","rlang","rstan","papaja")
```

We used `r cite_r("r-references.bib", footnote=T, withhold=F, pkgs=package_list)` for all our analyses.

# Results

In general groups showed expected patterns. They had high and increasing accuracy, coupled with faster response times, and decreases in utterance length showing the classic reduction pattern. 

```{r chat, include=F}
d.chat <- read_csv(here(data_location, "filtered_chat.csv")) %>% 
  filter(!is.chitchat) %>% 
  filter(!is.na(target)) %>% 
  mutate(text = gsub("\\n", '', fixed = T, text),
         text = gsub("[/?/.]", ' ', text),
         text = str_squish(text),
         tangram = gsub('/experiment/tangram_', '', target, fixed=TRUE),
         tangram = gsub('.png', '', tangram, fixed=TRUE),
         utt_length_chars = str_length(text), 
         utt_length_words = str_count(text, "\\W+") + 1) %>%
  group_by(gameId, trialNum, repNum, tangram, playerId, role, countCorrect, numPlayers) %>%
  summarize(text = paste0(text, collapse = ', '),
            total_num_words = sum(utt_length_words) %>% as.numeric(),
            total_num_chars = sum(utt_length_chars) %>% as.numeric()) %>%
  anti_join(rounds_exclude) %>% 
  full_join(d.round_results, c("gameId", "trialNum", "repNum", "playerId", "tangram", "countCorrect", "numPlayers")) %>% 
  mutate(text = text %|% "",
         total_num_words= total_num_words %|% 0,
         total_num_chars= total_num_chars %|% 0,
         role = role %|% "listener")

```
```{r, include=F}
model_input <- d.chat %>% filter(role=="speaker") %>% 
  mutate(block=repNum,
         words=total_num_words,
         tangram_group=str_c(tangram, gameId))
         
priors <- c(
  set_prior("normal(20, 20)", class="Intercept"),
  set_prior("normal(0, 10)", class="b"),
  set_prior("normal(0, 10)", class="sd"),
  set_prior("lkj(1)",       class="cor"))

model <- brm(words ~ block * numPlayers + (block|tangram)+ (1|playerId)+(1|tangram_group)+(block|gameId), data=model_input,file=here(model_location, "model1"),                            prior=priors, control=list(adapt_delta=.95))

         

```
```{r, include=F}
d.prev.speaker <- d.chat %>% ungroup() %>%  filter(role=="speaker") %>% select(gameId,repNum, tangram, total_num_words_prev=total_num_words)
d.prev.round <- d.chat %>% ungroup() %>% select(playerId, correct, tangram, gameId, repNum) %>% 
  left_join(d.prev.speaker) %>% unique() %>% mutate(repNum=repNum+1)


d.chat.lagged <- d.chat %>%
  ungroup() %>% 
  select(gameId, playerId, trialNum, repNum, playerId, role, tangram, total_num_words, numPlayers) %>%
  left_join(d.prev.round) %>%
  mutate(reduction_word=log(total_num_words)-log(total_num_words_prev)) %>%
  filter(repNum>0) %>%
  filter(role=="speaker") %>%
  mutate(prev_correct_round=correct)


model_input <- d.chat.lagged %>% filter(role=="speaker") %>% 
  mutate(block=repNum,
         words=total_num_words,
         tangram_group=str_c(tangram, gameId),
         was_INcorrect=ifelse(!prev_correct_round,1,0))
         
priors <- c(
  set_prior("normal(20, 20)", class="Intercept"),
  set_prior("normal(0, 10)", class="b"),
  set_prior("normal(0, 10)", class="sd"),
  set_prior("lkj(1)",       class="cor"))

model_speaker_acc <- brm(words ~ block * numPlayers +block*was_INcorrect+ (block|tangram)+ (1|playerId)+(1|tangram_group)+(block|gameId), data=model_input,file=here(model_location, "speaker_acc"),                         prior=priors, control=list(adapt_delta=.95))

         
```

```{r}

fem1 <- fixef(model)
fem2 <- fixef(model_speaker_acc)
```

(ref:accuracy) Listeners have high accuracy which increases of the course of the game, although accuracy increases less in 4-player games than smaller games. Accuracy rates are shown for each block, error bars are bootstrapped 95% CIs. 

```{r accuracy, out.height="30%", fig.width=6, fig.height=3, fig.pos='h', fig.cap="(ref:accuracy)", fig.align="center"}

d.round_results %>% group_by(playerId,repNum, gameId, numPlayers) %>% 
  mutate(correct.num=ifelse(correct,1,0)) %>% 
  ggplot(aes(x=repNum+1, y=correct.num, color=as.factor(numPlayers)))+
geom_smooth(method = "glm", method.args = list(family = "binomial")) + 
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width=.2))+
  #geom_point()+
  scale_color_brewer(palette="Dark2")+
  #scale_y_continuous(limits = c(0,1))+
  labs(x="Block Number", y="Fraction correct", color="Player count")+
    theme(legend.position="bottom")



```
Most groups were accurate in their selections, with accuracy rising over rounds (Fig \@ref(fig:accuracy)). This indicates that speakers were usually successful at conveying the intended referents. While on average games of each size increase in accuracy, 4-player games show lower gains in accuracy than smaller games. We do not have a clear explanation for why this is, whether it is reliable, or what pattern to expect for even larger (ex. 5 person) games. 


(ref:time) Listeners selected images faster in later blocks. Only times to correct responses are shown.

```{r time, out.height="30%", fig.width=6, fig.height=3, fig.pos='h', fig.cap="(ref:time)", fig.align="center"}

d.round_results %>% group_by(playerId, repNum, gameId, numPlayers) %>% 
  filter(correct==T) %>% 
  #summarize(time=mean(time)) %>% 
  ggplot(aes(x=repNum+1, y=time, color=as.factor(numPlayers)))+
  geom_jitter(width=.4, height=0, alpha=.03)+
geom_smooth(method = "glm", formula = y~x,
                      method.args = list(family = gaussian(link = 'log')))+
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width=.2))+
  scale_y_continuous(limits = c(0,180))+
    scale_color_brewer(palette="Dark2")+
  labs(x="Block Number", y="Time to selection in seconds", color="Player count")+
  theme(legend.position = "bottom")


```
Participants selected images faster in later rounds (Fig \@ref(fig:time)). There is wide variability, but appears to be an unintuitive effect of group size with fastest selection in 2-player games, but 4-player games being faster than 3-player games. This speed up is consistent with prior work by @weberCulturalConflictMerger2003 which used speed as the dependent measure.


(ref:total-words) Speaker and listeners say fewer words in later blocks. Note: y-axis clipped at 50 which hides a few speaker outliers. 

```{r total-words, out.height="30%", fig.width=6, fig.height=3, fig.pos='h', fig.cap="(ref:total-words)", fig.align="center"}

ggplot(d.chat, aes(x=repNum+1, y=total_num_words, color=as.factor(numPlayers)))+
  facet_wrap(~role, nrow=1)+
  scale_color_brewer(palette="Dark2")+
    geom_jitter(alpha=.05)+
    geom_smooth(method=glm, formula=y~poly(x,2), alpha=.3)+
     stat_summary(fun.data = "mean_cl_boot")+
    scale_y_continuous(limits = c(0,50))+
  labs(y="Number of words", x="Block number", color="Player count")+
  theme(legend.position="bottom")

```
The main effect of interest is whether speakers and listeners reduce in the number of words they say over the course of rounds. As shown in Fig \@ref(fig:total-words), the number of words produced does decrease. Listeners often don't talk much, but are more likely to ask questions or make clarification in early rounds. Speakers make longer utterances in early blocks and reduce to shorter utterances in later blocks. Notably, this shortening pattern occurs even as speakers rotate. In aggregate, the effect of being one block later is `r round(fem1["block","Estimate"],2)` [`r round(fem1["block", "Q2.5"],2)`, `r round(fem1["block","Q97.5"],2)`] words. The overall effect of having more players in a group is  `r round(fem1["numPlayers","Estimate"],2)` [`r round(fem1["numPlayers",  "Q2.5"],2)`, `r round(fem1["numPlayers","Q97.5"],2)`] per additional player. This estimate is uncertain because of a relatively small number of groups and wide group-level variability. 


(ref:word-lines) Words said by the speaker for each tangram in each group. Each referent/group trajectory is a thin line; aggregate curve is bolded. No outliers were omitted.

```{r variability, out.height="30%", fig.width=6, fig.height=3, fig.pos='h', fig.cap="(ref:word-lines)", fig.align="center"}

d.chat %>% filter(role=="speaker") %>% 
    mutate(groupxtangram=str_c(gameId,tangram)) %>% 
  group_by(repNum, numPlayers, gameId,tangram, groupxtangram) %>% 
  summarize(words=sum(total_num_words)) %>% 
ggplot(aes(x=repNum+1, y=words, color=as.factor(numPlayers)))+
  facet_wrap(~numPlayers, nrow=1)+
  scale_color_brewer(palette="Dark2")+
    geom_line(aes(group=groupxtangram), alpha=.1,method=glm, se=F)+
    geom_smooth(method = "glm", formula = y~x,method.args = list(family = gaussian(link = 'log')))+
  labs( y="Number of words", x="Block number", color="Player count")+
  theme(legend.position="null")


```
This variability can be seen in Fig \@ref(fig:variability). While the averaged data shows a smooth reduction in the number of words, individual trajectories for specific tangrams in specific groups are more varied. Reduction is not monotonic, as some later speakers use more words than were used in earlier rounds. 

Because the ground truth answers are not provided to listeners who make mistakes, they may not learn what utterance was referring to (unless they ask in the chat). What happens if a listener gets a tangram wrong and then is the speaker on the next block? For that tangram, they are unlikely to build off the previous descriptionsn and conventions that they don't understand. In contrast, a speaker who previously got the tangram right is reasonably likely to continue the conceptualization used so far and conventionalize it more, such as by reducing unneeded details. In aggregate hypothesizes that speakers should say more words if they got that tangram wrong the previous round that not, after controlling for other effects. This is borne out; speakers say `r round(fem2["was_INcorrect","Estimate"],2)` [`r round(fem2["was_INcorrect", "Q2.5"],2)`, `r round(fem2["was_INcorrect","Q97.5"],2)`] more words when previously wrong. 


# Discussion
TODO effects/interpretation of speaker rotation and feedback regime

The overall pattern of utterances shortening over repeated reference extends to groups of 3 or 4 people talking together and rotating between speaker and listener roles. 
Previous work (TODO CITATIONS) has varied in whether one person is the speaker the entire time or not. In rotating conditions, the interpretation of reduction is stronger evidence of conceptual agreement because more people have to agree to use the shorthand names, rather than just interpret them.

While some previous studies (TODO) have provided listeners with the correct answers when the made mistakes, we do not, merely informing them of their mistakes. This low level of feedback means that there isn't a way outside of the communication channel (and process of elimination) for people to find out what was meant for utterances they initially did not understand. Similarly, speakers don't have access to how well their partners did in the previous round (again, outside of explicit chat comments, or implicit assumptions drawn on the number of questions asked). 

Real-life communicative situations vary in what extra-textual feedback exists, but we do show that people can work around their initial confusion rather than just memorizing pairings after the first round. 


```{r}
total_words <- d.chat %>% ungroup() %>% filter(role=="speaker")%>% select(total_num_words) %>% summarize(words=sum(total_num_words)) %>% pluck()

total_referents <- d.chat %>% 
  ungroup()%>% filter(role=="speaker") %>% filter(total_num_words>0) %>% 
  select(trialNum,playerId) %>% unique() %>% tally()

total_speakers <- d.chat %>%  ungroup()%>% filter(role=="speaker") %>% filter(total_num_words>0) %>%
  select(playerId) %>% unique() %>% tally()
```

This is a rich data set consisting of `r signif(total_words,1)` words across `r signif(total_referents,1)` referring expressions by `r total_speakers` speakers, in addition to clarifications questions and comments from listeners. In this set of analyses, we rely on the easy to calculate measures of accuracy, speed, and word counts as proxies for the content of the utterances. In future analyses, it would be useful to do content analysis to understand how and when concepts are introduce and conventionalized and how much the semantics of utterances varies block to block (and speaker to speaker) depending on group size. 
A closer analysis of the utterances may yield information about how humans adapt language quickly, and the dataset may be useful for training artificial agents to use and understand language more dynamically. 


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
